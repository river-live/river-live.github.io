<!DOCTYPE html>

<html lang="en-US" prefix="og: http://opg.me/ns#">
  <head>
    <meta charset="UTF-8" />

    <meta name="title" property="og:title" content="River" />

    <meta
      name="description"
      property="og:description"
      content="River is a drop-in real-time service for web applications. It provides an easy-to-deploy and ready-to-scale solution for existing applications with real-time needs."
    />

    <meta name="type" property="og:type" content="website" />

    <meta name="url" property="og:url" content="https://river-live.github.io/" />

    <meta name="image" property="og:image" content="images/logos/river_logo-full.png" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta name="author" content="Catherine Emond, Mark Hyams" />

    <title>River</title>

    <link rel="apple-touch-icon" sizes="180x180" href="/images/icons/favicons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/icons/favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/icons/favicons/favicon-16x16.png">
    <link rel="manifest" href="/images/icons/favicons/site.webmanifest">
    <link rel="mask-icon" href="/images/icons/favicons/safari-pinned-tab.svg" color="#5bbad5">
    <link rel="shortcut icon" href="/images/icons/favicons/favicon.ico">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="msapplication-config" content="/images/icons/favicons/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">

    <!-- <style>reset</style> -->

    <link rel="stylesheet" href="stylesheets/reset.css" />

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/gruvbox-dark.min.css"
      charset="utf-8"
    />

    <!-- <style></style> -->

    <link rel="stylesheet" href="stylesheets/main.css" />

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

    <!-- <script></script> -->

    <script src="javascripts/application.js"></script>

    <style></style>
  </head>

  <body>
    <div class="logo-links">
      <p id="river-logo">MENU</p>

      <a href="https://github.com/river-live" target="_blank">
        <img src="images/logos/river_github-black.png" alt="github logo" id="github-logo" />
      </a>
    </div>
    <a id="toTop-link" href="#" target="_blank">
      <img src="images/logos/back-to-top.png" alt="Back to top" id="toTop-logo" />
    </a>
    <nav id="site-navigation">
      <ul>
        <li>
          <a href="#home" id="home-link">HOME</a>
        </li>

        <li>
          <a href="#case-study" id="case-study-link">CASE STUDY</a>

          <nav id="case-study-mobile">
            <ul></ul>
          </nav>
        </li>

        <li>
          <a href="#our-team" id="our-team-link">OUR TEAM</a>
        </li>
      </ul>
    </nav>

    <header id="home">
      <h1>
        <img src="images/logos/river_logo-full.png" alt="River logo" />
        <p>A drop-in real-time service for web applications</p>
      </h1>
    </header>

    <section class="integration">
      <div class="box">
        <img
          id="banner-deploy"
          src="images/diagrams//gifs/apex_query_logs_2.gif"
          alt="best practices"
        />
      </div>

      <article class="box">
        <div class="text-box">
          <h1>Centralized logging and tracing for your microservices</h1>

          <p>
            No additional client libraries needed in your service code
          </p>

        </div>
      </article>
    </section>

    <section class="integration">
      <article class="box">
        <div class="text-box">
          <h1>Manage custom fault-handling logic e.g. retries in one place</h1>

          <p>
            Define both global traffic rules and custom rules for individual services
          </p>

        </div>
      </article>

      <div class="box">
        <img
          id="banner-deploy"
          src="images/diagrams/5.4) orders-shipping custom configuration_2.png"
          class="softened"
          alt="Deploy Apex proxy with a few commands"
        />
      </div>
    </section>

    <section class="integration">
      <div class="box">
        <img
          id="banner-deploy"
          src="images/diagrams/gifs/apex_deploy_2.gif"
          class="softened"
          alt="Deploy Apex proxy with a few commands"
        />
      </div>

      <article class="box">
        <div class="text-box">
          <h1>Deploy to Docker containers with a few commands</h1>

          <p>
            No changes required in how your services are currently deployed
          </p>

        </div>
      </article>
    </section>

    <main>
      <section id="case-study">
        <h1>Case Study</h1>
        <!-- <p class="subheader">
          One place to log and control service-to-service traffic
        </p> -->

        <div id="side-nav">
          <img src="images/logos/river_logo-mark.png" alt="River logo" />
        </div>

        <nav>
          <ul></ul>
        </nav>

        <h2 id="what-is-river?">1) What is River?</h2>

        <p>
          River is a drop-in real-time service for web applications. It provides an easy-to-deploy and ready-to-scale solution for existing applications with real-time needs.
        </p>

        <p>
          For the application developer, River abstracts away the complexity of setting up a resilient infrastructure and removes the hassle of managing WebSocket connections. It allows backend services to publish events and makes these events available to web clients in real-time. River is a great fit for existing applications looking for a way to easily add real-time functionality.
        </p>

        <p>
          River is cloud-native. This means that it was designed and built with cloud services in mind. There are many buzzwords associated with cloud computing, but also a lot of very interesting ideas. Once the complexity hurdle has been crossed, cloud services give developers a lot of power and flexibility. Our goal with River was to leverage that power to provide others with a robust infrastructure to lean on and a clean interface to interact with it.
        </p>

        <p>
          In this case study, we describe how we designed and built River, the specific tradeoffs we made, and some of the technical challenges we encountered. But first, we will start with a quick overview of real-time in web applications.
        </p>

        <h2 id="what-is-real-time?">2) What is real-time?</h2>

        <h3>2.1) What is real-time in the context of web applications?</h3>

        <p>
          Real-time is a very wide space. At the outset, we want to narrow the focus and clarify what River is about. Real-time is about a fast exchange of messages. A change happens somewhere in our system and other parts of the system should be notified as fast as possible. What “as fast as possible” means, though, varies quite a lot depending on the context. Are we talking about microseconds latency? Is a missed deadline considered a system failure? For us, the answer to these questions is: no. We are in the context of web applications: this is not about an airplane control system.
        </p>

        <p>
          Even within the realm of web apps, some systems are much more sensitive to delays than others: gaming or financial stock trading are good examples. In this case, a higher latency can have serious consequences. These are sometimes called “firm” real-time. "Soft" real-time, on the other hand, is much more flexible. In that case, a higher latency will result in a degraded user experience, not a system failure. Collaborative apps are a good example. One can think of a collaborative text editor, such as Google docs, a discussion forum like Slack, or a shared calendar. The idea is that the real-time application should update to the correct state without user intervention [<a
          href="#footnote-1"
          >1</a
        >]. These are the type of applications we had in mind when building River.
        </p>

        <h3>2.2) How is real-time achieved?</h3>

        <p>
          In the past few decades, we have seen an evolution from hypertext documents and web pages to dynamic and interactive web applications. HTTP was not designed for web apps. HTTP was designed with simplicity in mind and is not an ideal choice for real-time communication [<a href="#footnote-2">2</a>].
        </p>

        <p>
          If we think of the HTTP request-response model, “without user intervention” would mean “without the client sending a request” and usually this is not an option. What we want is an open channel of communication through which messages can be sent between the server and the client, a bit more like an on-going conversation than a formal request. This bi-directional, message-based communication is such an appealing idea that it has given rise to a few different strategies and technologies over the years.
        </p>

        <div class="img-wrapper">
          <img
            src="example.png"
            alt="Monolith architecture vs microservices architecture"
          />
        </div>

        <h4>2.2.1) XHR Polling</h4>

        <p>
          The simplest form of XHR polling is sometimes called "short" polling: the client will simply make a request on a periodic interval to see if new data is available. There are, of course, quite a few downsides to this approach:
        </p>

        <ul>
          <li>
            The requests sent by the client are unnecessary if no new data is available. This can create extra load on the server.
          </li>
          <li>
            Every request and response sent will carry metadata overhead. For small and frequent messages, this is far from ideal.
          </li>
          <li>
            Depending on the HTTP version used, the TCP connection may need to be repeatedly established. This is normally avoided by using the <code>keepalive</code> connection header to establish a persistent connection (fn: A `keepalive` connection is the default since HTTP/1.1, Grigorik 2013, chap. 9) [<a href="#footnote-2">2</a>].
          </li>
        </ul>

        <p>
          Long-polling is an attempt to improve the situation by having the server "wait" after receiving the HTTP request, allowing it to respond only when there are new events to be sent. While it is an improvement, the general impression - as stated in the <a href="https://tools.ietf.org/html/rfc6202#section-2.2">RFC6202</a> - is that long polling "strech[es] the original semantics of HTTP and that the HTTP protocol was not designed for bidirectional communication".
        </p>

        <h4>2.2.2) Server-Sent Events</h4>

        <p>
          Server-Sent Events allow server-to-client streaming of text-based data. While they work over HTTP, they deviate from the usual request-response cycle and allow a truly message-based communication. This is sometimes referred to as server "push" to contrast it with a client "pull". As the name implies, after the connection is established, it is a one-way flow of data: the client cannot send messages to the server over that channel. With that limitation in mind, SSE work very well for delivering fast updates, such as push notifications. When considering SSE for River, the two main drawbacks were the unidirectional data flow and the limitation on the maximum number of open connections (fn: [MDN on SSE](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events). The limit per browser is set to 6 open connections. This is not an issue when using SSE with HTTP/2, since HTTP/2 allows multiplexing, [Denis 2016](https://www.infoq.com/articles/websocket-and-http2-coexist/)) [<a href="#footnote-3">3</a>].
        </p>

        <h4>2.2.3) WebSockets</h4>

        <p>
          WebSockets are widely adopted and used by popular sites such as GitHub and Slack. They are also what we chose to use with River. They are not new: the [RFC for the WebSocket protocol](https://tools.ietf.org/html/rfc6455) came out in 2011, precisely to solve the challenges of real-time communication. They are now supported by [all major browsers](https://caniuse.com/#search=WebSockets). WebSocket is a distinct protocol allowing for a persistent TCP connection between server and client (RFC6455).
        </p>

        <p>
          Through this open connection, WebSockets allow for efficient two-way data communication by minimizing metadata and avoiding the overhead of frequently opening and closing connections (fn: Lombardi 2015). Even more interesting, WebSockets give developers a lot of flexibility regarding the format of the messages sent. This can be good or bad - it has been described as some kind of "free for all" (fn: [Ornbo 2019](https://shapeshed.com/api-design-for-an-event-driven-world/)) [<a href="#footnote-4">4</a>]. But it allows us to layer on top of WebSockets higher-level protocols, such as the publish and subscribe model. This is something we leveraged when designing River and we will come back to it later.
        </p>

        <p>
          Two major pain points when working with WebSockets are:
        </p>

        <ul>
          <li>
            compatibility with existing infrastructure: WebSocket is a different protocol, distinct from HTTP.
          </li>
          <li>
            scale and security: many of the existing components of the infrastructure (e.g. servers and load balancers) are built, configured, and maintained with HTTP in mind (fn: [Denis 2016](https://www.infoq.com/articles/websocket-and-http2-coexist/)) [<a href="#footnote-5">5</a>].
          </li>
        </ul>

        <div class="img-wrapper">
          <img
            src="example.png"
            alt="Microservices communicate over the network"
          />
        </div>

        <h4>2.2.4) Metadata overhead</h4>

        <p>
          An interesting data point when looking at improving the performance of web applications is metadata overhead. These numbers are taken from the book High-Performance Browser Networking (fn: Grigorik 2013) [<a href="#footnote-6">6</a>]:
        </p>

        <ul>
          <li>
            WebSocket protocol: 2-14 bytes of overhead
          </li>
          <li>
            Server-Sent Events: 5 bytes of overhead
          </li>
          <li>
            HTTP 1.x: 500-800 bytes of HTTP metadata, plus cookies
          </li>
        </ul>

        <p>
          This is a significant difference, especially in the context of a fast exchange of messages.
        </p>

        <h4>2.2.5) HTTP/2</h4>

        <p>
          The work done on HTTP/2 explicitly addresses these issues. Its stated goal is to "enable a more efficient use of network resources and a reduced perception of latency" (RFC7540). To achieve that, HTTP/2 introduces header field compression, directly addressing the metadata overhead we just mentioned, as well as a push mechanism allowing servers to send resources to clients. Note that this "server push" is different from Server-Sent Events in that the resources are sent to the client cache, to be processed by the browser, not the application code (fn: [Denis, 2016](https://www.infoq.com/articles/websocket-and-http2-coexist/)) [<a href="#footnote-7">7</a>]. While HTTP/2 is not yet widely used (fn: [47% of all websites as of August 2020](https://w3techs.com/technologies/details/ce-http2)) [<a href="#footnote-8">8</a>], it shows that these are serious issues with the current model and worth taking into account.
        </p>

        <h3>2.3) Is real-time hard?</h3>

        <p>
          There is one final question we want to address in this section: how hard is it to build a real-time application? The answer, obviously, depends on the requirements of the application. Does it require data persistence? Does it need to scale? How secure should it be?
        </p>

        <p>
          Building a simple chat app is very easy to do: there are many open-source libraries that make working with WebSockets easier, such as socket.io. If an application developer wants to add real-time functionality to an existing codebase of a certain size, though, it could be another story. He might need to make a lot of modifications to the code and it can be worth considering putting that logic in a separate place. The same goes for building a resilient and scalable application: this is a new set of problems that could be difficult to handle if all the logic sits in one place. In the next section, we will explore the idea of a separate real-time service in more depth.
        </p>

        <h2 id="a-separate-real-time-service">3) A separate real-time service</h2>

        <h3>3.1) A separate server</h3>

        <p>
          As we mentioned in the beginning, River is a “drop-in” real-time service. This means that River is a separate component meant to be added to an existing application. Any time an extra piece is added to a system, it brings in some additional complexity. As a general rule of thumb, we should only add complexity when we really need it. It is a tradeoff that should be carefully considered and the need for a separate real-time service has to be justified. In this section, we want to start by looking more carefully at the drawbacks of using WebSockets that we mentioned in passing before.
        </p>

        <p>
          In the previous section, we presented the idea of persistent connections and how different it is from the usual request-response cycle. What we haven’t mentioned yet is that our server - a process - is now responsible for maintaining the open WebSocket connections. A stateless request-response cycle like HTTP and persistent connections like WebSockets have different technical requirements. These in turn can influence the configuration of the server and cause issues that will be amplified as the load on the server increases. This problem is often mentioned in the technical literature:
        </p>

        <blockquote>
          “HTTP is optimized for short and bursty transfers. As a result, many of the servers, proxies, and other intermediaries are often configured to aggressively timeout idle HTTP connections, which, of course, is exactly what we don’t want to see for long-lived WebSocket sessions.” (fn: Grigorik 2013) [<a href="#footnote-9">9</a>]
        </blockquote>

        <p>
          Having a separate server dedicated to maintaining WebSocket connections allows us to offload the management of persistent connections from our main application server. Again, this is common wisdom, as expressed here by Armin Ronacher, the creator of the Flask framework, in an article on lessons learned from using WebSockets:
        </p>

        <blockquote>
          “Keep your WebSockets miles away from the rest of your application code. Keep them in a separate server if you can. They have completely different performance characteristics than your regular HTTP requests.” (fn: [Ronacher 2012](https://lucumr.pocoo.org/2012/9/24/websockets-101/))[<a href="#footnote-10">10</a>]
        </blockquote>

        <p>
          This comes back to the well-known principle of separation of concerns: we have two different pieces of functionality, they do something different, and we should keep them in a separate place.
        </p>

        <h3>3.2) Multiple backend services</h3>

        <p>
          What if we need to work not only with one server but with multiple backend services? This is commonly referred to as a microservices architecture. There has been some pushback on the idea of microservices recently, mostly because it was sometimes presented as the solution to all problems, but most engineers seem to agree that it is a very useful model when working with big systems. With a microservices architecture, instead of working with one gigantic codebase, your application logic is split into multiple smaller services where each component can scale independently according to its needs.
        </p>

        <p>
          If we want to add real-time functionality to such a system, does it mean that each backend service must manage both HTTP and WebSocket connections? This would look something like this:
        </p>

        <div class="img-wrapper">
          <img
            src="example.png"
            alt="microservices-diagram-1"
          />
        </div>

        <p>
          Or probably more like this, since each backend service needs to maintain an open connection with each client if it wants to provide real-time functionality:
        </p>

        <div class="img-wrapper">
          <img
            src="example.png"
            alt="microservices-diagram-2"
          />
        </div>

        <p>
          What we really want is a separate real-time component responsible for managing the WebSocket connections and providing real-time functionality: 
        </p>

        <div class="img-wrapper">
          <img
            src="example.png"
            alt="microservices-diagram-3"
          />
        </div>

        <p>
          The backend services can now send normal HTTP requests to the real-time service. In a sense, they are now decoupled from the clients, as the real-time service is responsible for sending the messages to the clients.
        </p>

        <h3>3.3) Where does the real-time service fit in?</h3>

        <p>
          At a very high-level, what are the pieces of a real-time application?
        </p>

        <blockquote>
          “Real-time applications consist of clients, a real-time communication layer, and backend servers working together to achieve business objectives.” (Bussey 2020)[<a href="#footnote-11">11</a>]
        </blockquote>

        <p>
          We want to take a moment to go over a few communication patterns and see where that “real-time layer” fits in.
        </p>

        <h4>3.3.1) A browser-facing publish-subscribe system</h4>

        <p>
          We have mentioned before the terms “publish” and “subscribe” in the context of messaging. This is often referred to as the pub/sub messaging pattern. The exchange of messages can happen between backend services, or between the backend and the frontend. In all cases, it is a many-to-many relationship: multiple publishers can send events, while subscribers can subscribe to specific channels (sometimes called topics) in order to receive events from those channels. A middle layer allows the decoupling of publishers and subscribers: they do not need to be aware of each other. This layer simply needs to provide an interface for publishing, subscribing, and unsubscribing.
        </p>

        <p>
          While the pub/sub pattern is often associated with Event-Driven Architecture in the context of backend services, River is not an internal message broker like Apache Kafka. We find event streaming to be a very powerful model, but we had a more modest goal in mind: a real-time service responsible for managing WebSocket connections. Thus, it is important to keep in mind that we are talking about a browser-facing pub/sub system. The backend services publish events to the real-time service, which sends them to the client-subscribers - in this case, browsers. Implemented as a custom protocol on top of WebSocket connections, publish/subscribe is widely used (fn: cf. the [Pusher Protocol](https://pusher.com/docs/channels/library_auth_reference/pusher-websockets-protocol#subscription-events), [PubNub](https://www.pubnub.com/docs/platform/messages/publish), [Ably](https://www.ably.io/documentation/core-features/channels), and many more) [<a href="#footnote-12">12</a>] and remains a powerful pattern with a few obvious advantages.
        </p>

        <div class="img-wrapper">
          <img src="example.png" alt="browser-facing-pub-sub-diagram" />
        </div>

        <p>
          The pub/sub pattern:
        </p>

        <ul>
          <li>
            decouples the publishers from the subscribers
          </li>
          <li>
            allows multiple receivers for a single event
          </li>
          <li>
            allows a single client to subscribe to multiple channels
          </li>
          <li>
            provides developers with a simple interface for sending messages in a consistent format over the WebSocket protocol
          </li>
        </ul>

        <h4>3.3.2) The "triangular" pattern</h4>

        <p>
          The triangular pattern is a variation of the pub/sub pattern. Generally, events can be almost anything: a change in the database, or a message received from an external API. With this triangular pattern, messages sent by the client to the server over a normal HTTP request and response cycle are treated as events to be published. The real-time service is added to an existing application without the need to modify the existing HTTP communication. We will expand on this idea in the next section.
        </p>

        <div class="img-wrapper">
          <img src="example.png" alt="triangular-pattern-diagram" />
        </div>

        <h2 id="adding-real-time-functionality">4) Adding real-time functionality</h2>

        <p>
          In this section, we will look at what is needed to add real-time functionality to an existing application, by using a discussion forum as an example. A traditional discussion forum does not usually have real-time features, so to know if there is a new message in the forum, a user would need to refresh her browser. What are the steps needed to add this real-time functionality to an existing discussion forum, such that a user doesn’t have to refresh her browser to see new messages? We will examine how the request-response cycle changes (if at all), and how events can be sent to alert our users of new messages in real-time.
        </p>

        <p>
          Both of these solutions in fact share the same building block - a proxy server.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/4) Proxy server as building block.png"
            alt="The proxy server is a building block"
          />
        </div>

        <h3>4.1) Without River</h3>

        <p>
          A proxy is simply a server that sits on the path of network traffic between two
          communicating machines, and intercepts all their requests and responses. These machines
          could represent a client sending a request to another server, or for our purposes, two
          internal services communicating within the same architecture.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/4.0) Proxy server.png"
            alt="Proxy server can intercept and forward HTTP requests and responses"
          />
        </div>

        <p>
          In the above diagram, <code>orders</code> does not send an HTTP request directly to
          <code>shipping</code>; instead, it addresses its request to a host belonging to
          <code>proxy</code> (i.e. <code>proxy.com</code>). In order for <code>proxy</code> to know
          that <code>orders</code> actually wants to send its request to <code>shipping</code>,
          <code>orders</code> must specify <code>shipping</code>’s host (i.e.
          <code>shipping.com</code>) in another part of the request e.g. in the
          <code>Host</code> header value.
        </p>

        <p>
          When <code>proxy</code> receives a response back from <code>shipping</code>, it simply
          forwards the same response back to <code>orders</code>.
        </p>

        <h3>4.2) With River</h3>

        <h4>4.1.1) API gateway features</h4>

        <p>
          At its core, an API gateway is simply a proxy server (more precisely, a ‘reverse proxy’
          [<a href="#footnote-10">10</a>]). When used with microservices, one of its primary
          functions is to provide a stable API to clients and route client requests to the
          appropriate service. [<a href="#footnote-11">11</a>]
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/4.1.1) API gateway.png"
            alt="An API gateway proxies all incoming requests into a system"
          />
        </div>

        <p>
          It is certainly possible to deploy microservices without an API gateway. In such an
          architecture, whenever the client sends a request, it must already know which service to
          send the request to, and also the host and port of that service. This tightly couples the
          client with internal services, such that any newly added services, or updates to existing
          service APIs, must be deployed at the same time as updates in the client code. Such an
          architecture can be difficult to manage, as clients cannot always be relied upon to update
          immediately (e.g. mobile apps cannot be easily forced to update); even if they can, doing
          so would still incur additional engineering that could be avoided.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/4.1.1) Microservices without API gateway.png"
            alt="Without an API gateway, a client must know the host port and path of every service it needs to call"
          />
        </div>

        <p>
          With an API gateway, developers are largely free to update internal services while still
          providing a stable API to clients.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/4.1.1) API gateway's stable API.png"
            alt="An API gateway provides a stable API for clients, even if services are upgraded, replicated, or removed internally"
          />
        </div>

        <p>
          In addition to routing requests, the API gateway also provides one place to handle many
          concerns that are shared between services, such as authentication, caching, rate-limiting,
          load-balancing and monitoring.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/4.1.1) API gateway's features.png"
            alt="An API gateway also provides one place to manage other networking concerns"
          />
        </div>

        <p>
          In a way, an API gateway can be thought of as a receptionist at a large company. Any
          visitor does not necessarily have to know which employees are present in advance, or how
          different teams work together to complete specific tasks. Instead, they simply speak with
          the receptionist, who then decides, based on the visitor’s identity and stated purpose,
          which company employee to notify, and/or what access to grant to the visitor.
        </p>

        <h4>4.1.2) API gateway for service-to-service traffic?</h4>

        <p>
          Let us revisit the challenges that were described back in Section 3: 1) diagnosing faults
          in workflows that span multiple microservices, and 2) managing fault-handling logic that
          is similar across services.
        </p>

        <p>
          If the API gateway already provides one place to manage networking concerns, perhaps it is
          already a sufficient solution to these challenges? For example, instead of deploying it as
          a ‘front proxy’ that sits in front of all services, we could deploy it in a different
          pattern than it was intended for - as a proxy that sits between services internally. Would
          this not already provide the one place to log all service-to-service requests and
          responses, and define fault-handling logic like retries?
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/4.1.2) Deploying API gateway internally.png"
            alt="Could we just deploy an API gateway internally between services?"
          />
        </div>

        <p>
          In theory, this is certainly possible, but in practice, existing API gateway solutions are
          not ideal options for this.
        </p>

        <blockquote>
          <p>
            Optimized to handle [client-server] traffic at the edge of the data center, the API
            gateway ... is inefficient for the large volume of [service-to-service] traffic in
            distributed microservices environments: it has a large footprint (to maximize the
            solution’s appeal to customers with a wide range of use cases), can’t be containerized,
            and the constant communication with the database and a configuration server adds
            latency.
          </p>
        </blockquote>
        <p>
          <cite>
            - NGINX, maker of the popular open-source NGINX load balancer and web server [<a
              href="#footnote-12"
              >12</a
            >]
          </cite>
        </p>

        <p>
          In short, although the API gateway looks close to the solution we need, existing solutions
          on the market come built-in with many extra features that are designed for client-server
          traffic, making them a poor fit for managing service-to-service traffic.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/4.1.2) API gateway's extra features for client-server traffic.png"
            alt="Could we just deploy an API gateway internally between services?"
          />
        </div>

        <p>
          That is not to say a solution like an API gateway is completely out of the question. As we
          shall see in Section 5, the API gateway pattern was a major source of inspiration for
          Apex’s solution.
        </p>

        <h3>4.3) Overall pattern</h3>

        <p>
          The service mesh is another existing solution to the challenges with microservices that
          were outlined in Section 3. As mentioned previously, it also builds upon the proxy server.
        </p>

        <h4>4.2.1) Sidecar proxies</h4>

        <p>
          The service mesh is a highly complex solution, and we once again approach it through the
          analogy of a company. Consider a large team of people (analogous to services) who all
          communicate directly with each other.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/4.2.1) Everybody talks to everybody.png"
            alt="Communication in a large team: everybody talks to everybody"
          />
        </div>

        <p>
          As the team size grows, team members will likely find themselves spending more and more
          time handling these scenarios:
        </p>

        <ul>
          <li>
            <b>[Retry logic]</b> At any given time a team member may be off sick, so any other
            person who wishes to talk to them must retry again later.
          </li>
          <li>
            <b>[Rate-limiting]</b> A team member may be temporarily working reduced hours, and can
            only handle a limited number of incoming messages.
          </li>
          <li>
            <b>[Caching]</b> A team member may be asked for the same piece of information multiple
            times by other team members.
          </li>
          <li>
            <b>[Encryption]</b> Each team member is required to only use secure communication
            channels provided by the company.
          </li>
          <li>
            <b>[Authorization]</b> Some team members may be allowed to access confidential financial
            information, while others may not.
          </li>
          <li>
            <b>[Routing]</b> Sometimes a team member may need a particular piece of information, but
            does not know who has it, and so has to try several different people before obtaining
            it.
          </li>
          <li>
            <b>[Logging]</b> The company may wish to pull all messages from every team member’s
            inbox, to create a centralized record for auditing purposes.
          </li>
        </ul>

        <p>
          Managing these communication-related issues would take away time and focus from each team
          member’s core responsibilities.
        </p>

        <p>
          In this example, adding a service mesh is analogous to giving every team member a personal
          assistant (PA), who intercepts all incoming and outgoing messages and handles all the
          above tasks. This team structure would free team members from having to handle
          communication-related tasks, and allow them to focus more on their core responsibilities.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/4.2.1) Team members with their own PA.png"
            alt="Communication in a large team: Every team member talks through their own personal assistant"
          />
        </div>

        <p>
          In an actual service mesh, the PA would instead be a proxy server, known as a ‘sidecar
          proxy’. Each service is deployed alongside its own sidecar proxy, which intercepts all
          requests and responses to and from its parent service, and handles all the networking and
          infrastructure concerns we listed above, such as retry logic, rate-limiting etc. As a
          result, each service’s code can focus on its main business logic, while outsourcing
          networking and infrastructure concerns to the service’s sidecar proxy. [<a
            href="#footnote-13"
            >13</a
          >]
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/4.2.1) Services and sidecar proxies.png"
            alt="In a service mesh, services talk through their own 'sidecar proxies'"
          />
        </div>

        <h4>4.2.2) Configuration server</h4>

        <p>
          In addition to the sidecar proxies, the service mesh has one other important component - a
          central configuration server.
        </p>

        <p>
          Back in our hypothetical company, a configuration server is akin to a centralized folder
          containing data on team members and company policies e.g. who is on leave, who is working
          reduced hours, which secure channels to use, who has access to what information. Each
          personal assistant (PA) would have their own copy of this information to help them handle
          communication quickly, but whenever anything is updated in the centralized folder e.g. by
          the COO or HR Director, the changes are immediately sent to each PA, so that PAs always
          have the most up-to-date information in their own copies.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/4.2.2) Centralized folder with company policies and team info.png"
            alt="One centralized folder containing personnel info, with updates automatically copied to each PA's copy of the folder"
          />
        </div>

        <p>
          In the same way, the configuration server in a service mesh provides one place to update
          network traffic rules, such as logic for retries, caching, encryption, rate-limiting,
          routing. The configuration server is the source of truth for this information, but each
          sidecar proxy also has a cached copy of the information. Whenever the configuration server
          gets updated, it propagates the changes to each sidecar proxy, which then applies the
          changes to its own cached copy. [<a href="#footnote-14">14</a>]
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/4.2.2) Services and configuration server.png"
            alt="One configuration server containing all routes, retry logic, etc., with updates automatically pushed to each sidecar proxy's cached copy"
          />
        </div>

        <h4>4.2.3) Service mesh trade-offs</h4>

        <p>
          Again, let us revisit the challenges that were described back in Section 3: 1) diagnosing
          faults in workflows that span multiple microservices, and 2) managing fault-handling logic
          that is similar across services.
        </p>

        <p>
          The service mesh provides a robust solution to these challenges. The configuration server
          provides one place to define and update fault-handling logic; each sidecar proxy can be
          responsible for generating logs and sending them to one place to be stored, and also for
          executing fault-handling logic. Moreover, without any single point of failure or one
          single bottleneck, the architecture is resilient and highly scalable. [<a
            href="#footnote-15"
            >15</a
          >]
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/4.2.3) Service mesh as solution to microservices challenges.png"
            alt="Service mesh as solution to microservices challenges"
          />
        </div>

        <p>
          However, as with so many tools, rich functionality begets complexity. Implementing a full
          service mesh more than doubles the number of components in the architecture that must now
          be deployed and operated. In addition, both the sidecar proxy and its parent service are
          usually containerized to run alongside each other in the same virtual server. [<a
            href="#footnote-16"
            >16</a
          >] If any existing service is currently deployed without a container, then developers must
          now containerize it and redeploy it. More domain expertise must be acquired, and
          significant engineering effort expended.
        </p>

        <h3>4.3) Summary</h3>

        <p>
          As we have seen, solutions certainly exist to handle the challenges we described with
          microservices. Each existing solution embodies a different set of trade-offs.
        </p>

        <ul>
          <li>
            <b>API gateways’</b> features are designed for client-server, not service-to-service,
            traffic.
          </li>
          <li>
            <b>Service meshes</b> check all the boxes, but require teams to acquire more expertise,
            operate double the number of components, and redeploy existing services in a different
            pattern.
          </li>
        </ul>

        <div class="img-wrapper">
          <img
            src="images/diagrams/4.3) API gateway and service mesh trade-offs.png"
            alt="API gateway and service mesh trade-offs"
          />
        </div>

        <h2 id="design-architecture">5) Existing solutions</h2>

        <h3>5.1) Open-source solutions</h3>

        <p>
          For some teams, neither an API gateway nor a service mesh provide the right set of
          trade-offs. Consider a small team that are just beginning to migrate their monolith to
          include a few microservices. For ease of deployment, most of the services have been
          deployed to Heroku, or another platform as a service (PaaS) solution.
        </p>

        <p>
          It is likely that this team will have already experienced the challenges we mentioned back
          in Section 3: 1) diagnosing faults in workflows that span multiple microservices, and 2)
          managing fault-handling logic that is similar across services.
        </p>

        <p>
          For this team, a solution with the following trade-offs are needed:
        </p>

        <ul>
          <li>Optimized for handling service-to-service traffic</li>
          <li>One place to aggregate logs and manage traffic rules</li>
          <li>Simple to deploy and operate</li>
          <li>Fewer built-in features are acceptable</li>
          <li>
            Does not require changes to deployment pattern for existing services (e.g. does not
            require existing services to be containerized), since this may be difficult or not
            possible at all in PaaS solutions
          </li>
          <li>Lower availability is acceptable</li>
          <li>Lower scalability is acceptable</li>
        </ul>

        <div class="img-wrapper">
          <img src="images/diagrams/5.1) Apex trade-offs.png" alt="Apex trade-offs" />
        </div>

        <p>
          These are precisely the trade-offs we chose when building Apex.
        </p>

        <h3>5.2) Commercial solutions</h3>

        <p>
          Apex’s architecture includes 5 components:
        </p>

        <div class="img-wrapper">
          <img src="images/diagrams/5.2) Apex architecture.png" alt="Apex architecture" />
        </div>

        <ol>
          <li>Proxy server</li>
          <li>Logs database</li>
          <li>Configuration store</li>
          <li>Admin API</li>
          <li>Admin UI</li>
        </ol>

        <p>
          Apex’s core component, the <code>apex-proxy</code> server sits on the path of network
          traffic between every pair of communicating microservices, such as that between
          <code>orders</code> and <code>shipping</code> above. In the case of systems with more than
          two services, the following diagram shows how Apex would be deployed.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/5.2) Mediating all service-to-service traffic.png"
            alt="Apex mediating all service-to-service traffic"
          />
        </div>

        <p>
          Recall that an API gateway is just a proxy server that handles all client-server traffic
          coming into a system, and routes client requests to the correct service. In a similar way,
          Apex can be thought of as a stripped-down, internally deployed API gateway, which routes
          not traffic between clients and servers, but traffic between services.
        </p>

        <p>
          Zooming further into <code>apex-proxy</code>, there are several middleware layers that
          each provide additional functionality beyond simple proxying, such as authentication,
          routing, retries and logging.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/5.2) Apex architecture with middleware.png"
            alt="Apex architecture with middleware"
          />
        </div>

        <h3>5.3) DAZN in-house WebSocket solution</h3>

        <p>
          Since <code>apex-proxy</code> intercepts all network traffic between microservices, it is
          able to aggregate logs for every request and response, and send them to
          <code>apex-logs-db</code> to be persisted and queried in one place.
        </p>

        <div class="img-wrapper">
          <img src="images/diagrams/5.3) apex-logs-db.png" alt="Apex logs database" />
        </div>

        <p>
          Additionally, <code>apex-proxy</code> provides the ability to trace requests and responses
          that belong to the same request-response cycle. Any request that comes into
          <code>apex-proxy</code> is given an extra <code>correlation-id</code> HTTP header value
          (<code>f84nw2</code> in the example diagram below), if it doesn’t have one already, before
          being logged. This same <code>correlation-id</code> value is then also included as the
          request is forwarded to the responding service. When a response comes back from the
          responding service, Apex adds this same <code>correlation-id</code> value to the response,
          before forwarding this updated response back to the requesting service. As a result, all
          requests and responses belonging to the same request-response cycle have the same
          <code>correlation-id</code> value when they are logged, making it easy to query them
          together.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/5.3) Adding correlation_id header.png"
            alt="Requests and responses in the same cycle are given the same correlation_id"
          />
        </div>

        <p>
          This same feature also makes it possible to connect requests and responses belonging to
          workflows that span multiple services. As long as each service adds some logic to
          propagate any <code>correlation-id</code> header value that already exists in incoming
          requests, then all requests and responses belonging to the same workflow will have the
          same <code>correlation-id</code> value in <code>apex-logs-db</code>.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/5.3) Tracing multi-service workflow.png"
            alt="In a multi-service workflow, requests and responses can be traced by correlation_id"
          />
        </div>

        <p>
          Now, figuring out where a request failed within a workflow is just a matter of querying
          <code>apex-logs-db</code> for that one <code>correlation-id</code> value. This solves our
          first problem of diagnosing faults in workflows that span multiple microservices.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/5.3) Querying logs by correlation_id.png"
            alt="The admin UI allows logs to be queried by correlation_id"
          />
        </div>

        <p>
          Below, we demonstrate this feature on an actual deployed instance of Apex.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/gifs/apex_query_logs_2.gif"
            alt="Monolith with multiple databases"
          />
        </div>

        <h3>5.4) One place to manage fault-handling logic</h3>

        <p>
          Similar to the service mesh, Apex also has a single configuration server, the
          <code>apex-config-store</code>, where developers can define logic for retries, routing
          etc. In this way, Apex can be thought of as a stripped-down service mesh.
        </p>

        <div class="img-wrapper">
          <img src="images/diagrams/5.4) apex-config-store.png" alt="Apex configuration store" />
        </div>

        <p><code>apex-config-store</code> contains the following configuration data:</p>

        <ul>
          <li>
            <code>service-credentials</code> is used for authentication. It stores the list of
            service names along with their passwords. Every service that sends a request to Apex
            must authenticate itself with a token generated using its name and password.
          </li>
          <li>
            <code>service-hosts</code> is used for routing. It lists the IP address or domain name
            where each service can be found.
          </li>
          <li>
            <code>default-default</code> is used for defining global defaults for service-to-service
            traffic. In the above diagram, currently by default a request times out if a response is
            not received within 3,500 ms, and can be retried a maximum of 4 times. Each new retry
            attempt must wait, or ‘back off’, for 5,000 ms after the last failed request.
          </li>
          <li>
            <code>orders-shipping</code> and <code>shipping-inventory</code> are examples of
            service-specific rules that override the global defaults:
            <ul>
              <li>
                Whenever <code>orders</code> sends a request to <code>shipping</code>, requests time
                out after 5,000 ms, and can only be retried no more than 2 times, with a backoff of
                2500 ms.
              </li>
              <li>
                When <code>shipping</code> sends a request to <code>inventory</code>, however,
                requests time out after 2,000 ms, and there shall be no retries at all.
              </li>
            </ul>
          </li>
        </ul>

        <p>
          For every request sent to Apex, <code>apex-config-store</code> is queried for
          authentication, routing and retry logic - in that order. Only after all three are complete
          do requests get forwarded onto the responding service.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/5.4) apex-proxy + apex-config-store.png"
            alt="Apex proxy relies heavily on the configuration store"
          />
        </div>

        <p>
          With this one place to define and update configuration data, Apex’s architecture provides
          a solution to our second problem of managing fault-handling logic (as well as other
          network concerns) that is often similar across services.
        </p>

        <p>
          Below is the actual Apex UI for defining retry logic for when the
          <code>orders</code> service calls the shipping service.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/5.4) orders-shipping custom configuration_2.png"
            alt="Custom configuration logic for when the orders service contacts the shipping service"
          />
        </div>

        <p>
          Applying the same company analogy that we used for the service mesh, Apex is comparable to
          having just one team assistant (<code>apex-proxy</code>) for the whole team, rather than
          one personal assistant per team member, mediate all communication between team members.
          Every time any team member needs to communicate with another team member, they send their
          messages through the team assistant. On every incoming message, the team assistant checks
          a centralized folder (<code>apex-config-store</code>) containing all the relevant
          information on team members and company policies, to verify the identity of the sender,
          determine who should receive the message, as well as how many times to retry should the
          first attempt fail.
        </p>

        <h3>5.5) Trading off availability and scalability</h3>

        <p>
          Though Apex provides a solution to the two microservices challenges that were described
          back in Section 3, it comes with trade-offs, namely lower availability, and lower
          scalability.
        </p>

        <p>
          One of the strengths of the full service mesh is that there is no one component that sits
          on the path of all service-to-service traffic. If a sidecar proxy crashes or gets
          overloaded, only its parent service becomes unavailable, while the remaining services can
          continue to operate normally. With Apex, however, the <code>apex-proxy</code> becomes a
          single point of failure and traffic bottleneck. Any outage in <code>apex-proxy</code> will
          halt all service-to-service traffic and render the entire system unavailable.
        </p>

        <p>
          Ultimately, there is an inherent trade-off between the number of proxies in the system
          (and hence availability and scalability), and how easy it is to deploy and operate the
          system. Apex and service meshes occupy opposite ends of this spectrum.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/5.5) Apex and service meshes on spectrum.png"
            alt="Apex is easier to deploy and operate, while service meshes offer higher availability and scalability"
          />
        </div>

        <h3>5.6) Transitioning to a service mesh</h3>

        <p>
          Despite the seemingly divergent set of trade-offs between Apex and the service mesh,
          Apex’s architecture is in fact acknowledged by several service mesh vendors as a possible
          transitional architecture on the journey toward a full service mesh. NGINX calls this
          architecture a ‘Router Mesh’ [<a href="#footnote-17">17</a>]; Citrix calls it a ‘Service
          Mesh Lite’ [<a href="#footnote-18">18</a>], and Kong calls it an ‘Internal API gateway’
          [<a href="#footnote-19">19</a>].
        </p>

        <p>
          Therefore, any team that adopts Apex’s architecture can rest assured that they are not
          taking a path that is mutually exclusive to eventually adopting a full service mesh. The
          truth is quite the opposite - this architecture is “relatively easy to implement,
          powerful, efficient, and fast”, and forms part of a “progression” toward a service mesh
          [<a href="#footnote-20">20</a>].
        </p>

        <h2 id="implementation-deployment">6) Building River</h2>

        <p>
          When implementing Apex, we made technology choices based on the trade-offs we described in
          Section 5.1. In particular, we prioritized ease of deployment and operation over
          feature-richness, high availability and high scalability. The technologies we ended up
          choosing include Node.js and Express.js, TimescaleDB, Redis, React and Docker.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/6) Apex architecture with technologies.png"
            alt="The Apex core components use several technologies"
          />
        </div>

        <p>
          Below, we briefly elaborate upon each of these choices.
        </p>

        <h3>6.1) Designing River</h3>

        <div class="img-wrapper">
          <img
            src="images/diagrams/6.1) apex-proxy in Node and Express.png"
            alt="Apex proxy uses Node.js and Express.js"
          />
        </div>

        <p>
          For the main proxy server, we had the choice between using any popular web development
          framework (e.g. Rails), and building atop an existing proxy (e.g. Envoy, NGINX). Since one
          of our design goals was to be ‘simply to deploy and operate’, we preferred a solution that
          did not come built-in with any extra features that are irrelevant to our target user. With
          this in mind, we decided on the “fast, unopinionated, minimalist” Express.js framework [<a
            href="#footnote-21"
            >21</a
          >] built in Node.js, a language known for its ability to “handle a huge number of
          simultaneous connections with high throughput” [<a href="#footnote-22">22</a>] and
          widespread usage among developers.
        </p>

        <h3>6.2) Building, piece-by-piece</h3>

        <div class="img-wrapper">
          <img
            src="images/diagrams/6.2) apex-logs-db in TimescaleDB.png"
            alt="The log database is an instance of TimescaleDB"
          />
        </div>

        <p>
          The request and response logs generated by <code>apex-proxy</code> are a type of
          time-series data. [<a href="#footnote-23">23</a>] To store them in one place, we chose
          TimescaleDB, a time-series database that can ingest data at a rate of more than 100,000
          rows per second, even as a database reaches billions of rows [<a href="#footnote-24">24</a
          >]. This high ingestion rate mitigates the risk that writing logs to storage will become a
          bottleneck in the system.
        </p>

        <h3>6.3) The road not taken</h3>

        <div class="img-wrapper">
          <img
            src="images/diagrams/6.3) apex-config-store in Redis.png"
            alt="A Redis key-value store holds configuration data"
          />
        </div>

        <p>
          One of Apex’s core features is providing service owners with one place to modify service
          information (e.g. register their service, generate new credentials for authentication),
          and update fault-handling logic (e.g. retry logic) for their own service. To enable this,
          we had several options for where to store the configuration data: 1) in an environment
          file that is loaded into memory when <code>apex-proxy</code> spins up, 2) in a file that
          is read by <code>apex-proxy</code> for every request, or 3) in an external configuration
          data store.
        </p>

        <p>
          Option 1 of using an environment file was immediately ruled out, as it requires that the
          <code>apex-proxy</code> process be restarted every time the file is updated. Between the
          two remaining options, storing configuration in a file on disk leads to faster reads,
          since in general disk IO is faster than fetching data over the network. However, files can
          be easily corrupted, if say multiple processes write to the same file at the same time.
        </p>

        <p>
          In the end, we decided on Option 3, and implemented a Redis key-value store that gets
          queried for configuration data on every request. Redis stores all its data in memory, and
          so enables reads at over 72,000 requests per second [<a href="#footnote-25">25</a>]. This
          somewhat makes up for Option 3’s slower read speed compared to Option 2. In addition,
          Redis persists data to disk once every second, ensuring that configuration data will
          remain intact even if the Redis instance crashes and must restart.
        </p>

        <h3>6.4) <code>apex-admin-api</code> - Node.js and Express.js</h3>

        <div class="img-wrapper">
          <img
            src="images/diagrams/6.4) apex-admin-api in Node and Express.png"
            alt="The admin API is another Node and Express application"
          />
        </div>

        <p>
          For convenience, we built a REST API that enables users to programmatically query their
          logs in TimescaleDB and update config data in Redis (as opposed to having to SSH into
          those instances and issue commands in the terminal). This API also provides the option for
          admins to build additional UIs for different access roles e.g. a logs-only UI for users
          who are not authorized to update configuration data.
        </p>

        <h3>6.5) <code>apex-admin-ui</code> - React</h3>

        <div class="img-wrapper">
          <img
            src="images/diagrams/6.5) apex-admin-ui in React.png"
            alt="The admin user interface was built with React"
          />
        </div>

        <p>
          Finally, <code>apex-admin-ui</code> communicates with the
          <code>apex-admin-api</code> backend and provides service owners with a convenient way to
          register new services, edit existing service information, add and edit custom
          configuration, and query logs by <code>correlation_id</code>.
        </p>

        <h3>6.6) Deployment - Docker and Docker Compose</h3>

        <div class="img-wrapper">
          <img
            src="images/diagrams/6.6) Deploying in Docker containers.png"
            alt="Each of the core components deploys in its own Docker container"
          />
        </div>

        <p>
          Installing and running five interconnected components will likely be a time-consuming
          process fraught with unpredictable environment-specific errors. Standing by our design
          goal of being ‘simple to deploy and operate’, Apex’s components are all containerized
          using Docker, and deployed in a coordinated fashion with Docker Compose. This ensures
          Apex’s components are all deployed in the same (containerized) environment for every user.
        </p>

        <p>
          As shown below, deploying Apex with Docker Compose locally requires just one
          <code>docker-compose up</code> command.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/gifs/apex_deploy_2.gif"
            alt="Each of the core components deploys in its own Docker container"
          />
        </div>

        <p>
          Apex’s documentation also provides step-by-step instructions for deploying Apex to AWS’s
          Elastic Container Service (ECS).
        </p>

        <h2 id="implementation-challenges">7) Automating deployment</h2>

        <h3>7.1) Interacting with AWS</h3>

        <p>
          Sending large request and response bodies from <code>apex-proxy</code> to
          <code>apex-logs-db</code> can add significant latencies to request-response cycles, spin
          up long-running processes in <code>apex-proxy</code> that decrease its throughput, and
          fill up <code>apex-logs-db</code> far faster than necessary.
        </p>

        <p>
          To solve these problems, we ultimately chose to avoid decompressing any log bodies that
          arrive in a compressed format, and send logs asynchronously from an in-memory queue.
        </p>

        <h4>7.1.1) Solution 1: keep bodies compressed</h4>

        <p>
          Quite simply, sending compressed bodies means fewer bytes transmitted and stored. For a
          typical web page that arrives at <code>apex-proxy</code> in a compressed format (e.g.
          CNN’s homepage), we found that sending the compressed body to
          <code>apex-logs-db</code> typically took less than 1 second, compared with 5-10 seconds
          for the decompressed version.
        </p>

        <p>
          While keeping bodies compressed was a sensible choice, it came with the trade-off of
          inconvenience for users of <code>apex-logs-db</code>, who must now take the extra step to
          decompress bodies to make them human-readable again.
        </p>

        <h4>7.1.2) Solution 2: queue logs and send them asynchronously</h4>

        <p>
          Queuing logs to be sent asynchronously has the effect of decoupling writes to
          <code>apex-logs-db</code> from request-response cycles through <code>apex-proxy</code>. If
          <code>apex-proxy</code> happens to receive a particularly large response body that must be
          logged, it can simply enqueue this log, and move on to forwarding the response back to the
          requesting service and then on to processing the next request. The request-response cycle
          can complete regardless of when, or whether, the log eventually gets sent to
          <code>apex-logs-db</code>.
        </p>

        <p>
          Adding a queue in this way also lays the foundation for a further optimization - sending
          logs to TimescaleDB in batches. TimescaleDB’s own docs explain that this could further
          increase its data ingestion rate. [<a href="#footnote-26">26</a>]
        </p>

        <p>
          However, this solution comes with two trade-offs. The first is that several large log
          queues within concurrent processes could consume a lot of memory, straining the host
          server. Given TimescaleDB’s high ingestion rate, we made the decision to accept this
          trade-off, in the belief that the logs will dequeue fast enough to avoid hitting such a
          limit.
        </p>

        <p>
          The second trade-off is that should <code>apex-proxy</code> crash, any logs that have not
          yet been dequeued would now be lost from memory. Since each individual log is relatively
          unimportant data, we also deemed this an acceptable trade-off.
        </p>

        <h3>7.2) Creating a Lambda: an example</h3>

        <p>
          While containerizing TimescaleDB and Redis made deployment simpler for users, it also
          increased the risk of losing logs and configuration data. This is due to the ephemeral
          nature of Docker containers, whose “data doesn’t persist when the container no longer
          exists”. [<a href="#footnote-27">27</a>]
        </p>

        <p>
          Fortunately, Docker containers support ‘volumes’, a mechanism to persist data to a
          container’s host filesystem, and beyond the container’s lifespan. [<a href="#footnote-28"
            >28</a
          >] When deploying containers locally with Docker Compose, enabling this feature requires
          just an extra line of code in the <code>docker-compose.yml</code> configuration file.
        </p>

        <p>
          Deploying Docker containers to AWS’s Elastic Container Service (ECS), though, requires
          more care. ECS offers two launch types [<a href="#footnote-29">29</a>]: the EC2 launch
          type provides more control, by allowing developers to choose the type and quantity of EC2
          instances to provision for their containers. Its downside is that it requires more steps
          to deploy. The Fargate launch type, in contrast, abstracts away the entire
          resource-provisioning process, reducing the deployment process to running just 6 or so
          commands. Crucially, only the EC2 launch type supports Docker volumes. [<a
            href="#footnote-30"
            >30</a
          >]
        </p>

        <p>
          We had initially wanted to support the Fargate launch type, in alignment with our design
          goal of being ‘simple to deploy’. However, it was clear to us that the ability to persist
          logs and configuration data beyond the lifecycle of individual containers will be
          important for any Apex user, and in the end we spent a significant amount of extra time
          configuring Apex to support the EC2 launch type.
        </p>

        <h2 id="future-work">8) How to use River</h2>

        <h3>8.1) Deployment</h3>

        <p>
          Apex, as is, represents a single point of failure for a system. To protect it from being
          overwhelmed by bursty traffic, a best practice is to deploy a FIFO (‘first in, first out’)
          queue in front of <code>apex-proxy</code>. Another option we are considering is to deploy
          a standard queue without FIFO guarantees, which offers higher throughput rates, but
          requires additional middleware in <code>apex-proxy</code> to ensure messages are consumed
          in the right order.
        </p>

        <h3>8.2) Libraries</h3>

        <p>
          While Docker volumes offer strong persistence guarantees for Apex’s logs and configuration
          data, the compute instances (e.g. AWS EC2) hosting the containers are ephemeral and not
          suitable for long-term storage. For users who need even stronger persistence guarantees,
          <code>apex-logs-db</code> and <code>apex-config-store</code> can be configured to
          periodically back up data to a cloud storage service (e.g. AWS S3 and S3 Glacier).
        </p>

        <h3>8.3) Authentication</h3>

        <p>
          Currently, every incoming request to Apex triggers multiple reads from the Redis
          <code>apex-config-store</code>. As Apex is designed for small teams with a finite and
          small number of services, most of these reads from Redis will be for the same
          configuration data. Therefore, we can significantly reduce the read rate by caching a copy
          of all configuration data in memory within <code>apex-proxy</code>. This of course creates
          the new problem of cache invalidation - any changes in Redis must now be propagated to
          <code>apex-proxy</code> e.g. through an extra REST endpoint in <code>apex-proxy</code>.
        </p>

        <section id="footnotes">
          <h2 id="references">9) References</h2>

          <ol>
            <li id="footnote-1">
              <a
                href="https://pragprog.com/titles/sbsockets/real-time-phoenix/"
                target="_blank"
                >S. Bussey, Real-Time Phoenix: Build Highly Scalable Systems with Channels</a
              >
            </li>
            <li id="footnote-2">
              <a
                href="https://hpbn.co/"
                target="_blank"
                >I. Grigorik, High Performance Browser Networking</a
              >
            </li>
            <li id="footnote-3">
              <a href="https://samnewman.io/books/monolith-to-microservices/" target="_blank"
                >https://samnewman.io/books/monolith-to-microservices/</a
              >
            </li>
            <li id="footnote-4">
              <a
                href="https://martinfowler.com/articles/microservice-trade-offs.html#diversity"
                target="_blank"
                >https://martinfowler.com/articles/microservice-trade-offs.html#diversity</a
              >
            </li>
            <li id="footnote-5">
              <a
                href="https://martinfowler.com/articles/microservice-trade-offs.html#deployment"
                target="_blank"
                >https://martinfowler.com/articles/microservice-trade-offs.html#deployment</a
              >
            </li>
            <li id="footnote-6">
              <a
                href="https://martinfowler.com/articles/microservices.html#AreMicroservicesTheFuture"
                target="_blank"
                >https://martinfowler.com/articles/microservices.html#AreMicroservicesTheFuture</a
              >
            </li>
            <li id="footnote-7">
              <a
                href="https://www.oreilly.com/library/view/the-enterprise-path/9781492041795/"
                target="_blank"
                >https://www.oreilly.com/library/view/the-enterprise-path/9781492041795/</a
              >
            </li>
            <li id="footnote-8">
              <a href="https://stripe.com/docs/libraries" target="_blank"
                >https://stripe.com/docs/libraries</a
              >
            </li>
            <li id="footnote-9">
              <a
                href="https://www.oreilly.com/library/view/the-enterprise-path/9781492041795/"
                target="_blank"
                >https://www.oreilly.com/library/view/the-enterprise-path/9781492041795/</a
              >
            </li>
            <li id="footnote-10">
              <a
                href="https://www.cloudflare.com/learning/cdn/glossary/reverse-proxy/"
                target="_blank"
                >https://www.cloudflare.com/learning/cdn/glossary/reverse-proxy/</a
              >
            </li>
            <li id="footnote-11">
              <a
                href="https://www.nginx.com/blog/building-microservices-using-an-api-gateway/"
                target="_blank"
                >https://www.nginx.com/blog/building-microservices-using-an-api-gateway/</a
              >
            </li>
            <li id="footnote-12">
              <a
                href="https://www.nginx.com/blog/do-you-really-need-different-kinds-of-api-gateways-hint-no/"
                target="_blank"
                >https://www.nginx.com/blog/do-you-really-need-different-kinds-of-api-gateways-hint-no/</a
              >
            </li>
            <li id="footnote-13">
              <a
                href="https://www.oreilly.com/library/view/the-enterprise-path/9781492041795/"
                target="_blank"
                >https://www.oreilly.com/library/view/the-enterprise-path/9781492041795/</a
              >
            </li>
            <li id="footnote-14">
              <a
                href="https://www.oreilly.com/library/view/the-enterprise-path/9781492041795/"
                target="_blank"
                >https://www.oreilly.com/library/view/the-enterprise-path/9781492041795/</a
              >
            </li>
            <li id="footnote-15">
              <a
                href="https://www.nginx.com/blog/microservices-reference-architecture-nginx-fabric-model/"
                target="_blank"
                >https://www.nginx.com/blog/microservices-reference-architecture-nginx-fabric-model/</a
              >
            </li>
            <li id="footnote-16">
              <a href="https://www.nginx.com/blog/do-i-need-a-service-mesh/" target="_blank"
                >https://www.nginx.com/blog/do-i-need-a-service-mesh/</a
              >
            </li>
            <li id="footnote-17">
              <a
                href="https://www.nginx.com/blog/microservices-reference-architecture-nginx-router-mesh-model/"
                target="_blank"
                >https://www.nginx.com/blog/microservices-reference-architecture-nginx-router-mesh-model/</a
              >
            </li>
            <li id="footnote-18">
              <a
                href="https://thenewstack.io/part-4-when-a-service-mesh-lite-proxy-is-right-for-your-organization/"
                target="_blank"
                >https://thenewstack.io/part-4-when-a-service-mesh-lite-proxy-is-right-for-your-organization/</a
              >
            </li>
            <li id="footnote-19">
              <a href="https://konghq.com/blog/kong-service-mesh/" target="_blank"
                >https://konghq.com/blog/kong-service-mesh/</a
              >
            </li>
            <li id="footnote-20">
              <a
                href="https://www.nginx.com/blog/microservices-reference-architecture-nginx-router-mesh-model/"
                target="_blank"
                >https://www.nginx.com/blog/microservices-reference-architecture-nginx-router-mesh-model/</a
              >
            </li>
            <li id="footnote-21">
              <a href="https://expressjs.com/" target="_blank">https://expressjs.com/</a>
            </li>
            <li id="footnote-22">
              <a
                href="https://www.toptal.com/nodejs/why-the-hell-would-i-use-node-js"
                target="_blank"
                >https://www.toptal.com/nodejs/why-the-hell-would-i-use-node-js</a
              >
            </li>
            <li id="footnote-23">
              <a
                href="https://blog.timescale.com/blog/what-the-heck-is-time-series-data-and-why-do-i-need-a-time-series-database-dcf3b1b18563/"
                target="_blank"
                >https://blog.timescale.com/blog/what-the-heck-is-time-series-data-and-why-do-i-need-a-time-series-database-dcf3b1b18563/</a
              >
            </li>
            <li id="footnote-24">
              <a
                href="https://docs.timescale.com/latest/introduction/timescaledb-vs-postgres"
                target="_blank"
                >https://docs.timescale.com/latest/introduction/timescaledb-vs-postgres</a
              >
            </li>
            <li id="footnote-25">
              <a href="https://redis.io/topics/benchmarks" target="_blank"
                >https://redis.io/topics/benchmarks</a
              >
            </li>
            <li id="footnote-26">
              <a
                href="https://docs.timescale.com/latest/using-timescaledb/ingesting-data"
                target="_blank"
                >https://docs.timescale.com/latest/using-timescaledb/ingesting-data</a
              >
            </li>
            <li id="footnote-27">
              <a href="https://docs.docker.com/storage/" target="_blank"
                >https://docs.docker.com/storage/</a
              >
            </li>
            <li id="footnote-28">
              <a href="https://docs.docker.com/storage/volumes/" target="_blank"
                >https://docs.docker.com/storage/volumes/</a
              >
            </li>
            <li id="footnote-29">
              <a
                href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html"
                target="_blank"
                >https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html</a
              >
            </li>
            <li id="footnote-30">
              <a
                href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-volumes.html#docker-volume-considerations"
                target="_blank"
                >https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-volumes.html#docker-volume-considerations</a
              >
            </li>
          </ol>

          Icons made by
          <a href="https://www.flaticon.com/authors/freepik" title="Freepik" target="_blank"
            >Freepik</a
          >,
          <a href="https://www.flaticon.com/authors/those-icons" title="Those Icons" target="_blank"
            >Those Icons</a
          >,
          <a
            href="https://www.flaticon.com/authors/kiranshastry"
            title="Kiranshastry"
            target="_blank"
            >Kiranshastry</a
          >,
          <a href="https://www.flaticon.com/authors/smashicons" title="Smashicons" target="_blank"
            >Smashicons</a
          >,
          <a
            href="https://www.flaticon.com/authors/pixel-perfect"
            title="Pixel perfect"
            target="_blank"
            >Pixel perfect</a
          >, and
          <a href="https://www.flaticon.com/authors/eucalyp" title="Eucalyp" target="_blank"
            >Eucalyp</a
          >
          from
          <a href="https://www.flaticon.com/" title="Flaticon" target="_blank"> www.flaticon.com</a
          >.
        </section>
      </section>
    </main>

    <section id="our-team">
      <h1>Our Team</h1>

      <p>
        We are looking for opportunities. If you liked what you saw and want to talk more, please
        reach out!
      </p>

      <ul>
        <li class="individual">
          <img
            src="https://avatars1.githubusercontent.com/u/10019150?s=460&u=916f068a14b51f7f173c26943a966a49642bbdc4&v=4"
            alt="Derick Gross"
          />

          <h3>Derick Gross</h3>

          <p>New York, NY</p>

          <ul class="social-icons">
            <li>
              <a href="mailto:derick.gross@gmail.com" target="">
                <img src="images/icons/email_icon.png" alt="email" />
              </a>
            </li>

            <li>
              <a href="https://github.com/derickgross" target="_blank">
                <img src="images/icons/website_icon.png" alt="website" />
              </a>
            </li>

            <li>
              <a href="https://www.linkedin.com/in/derickgross/" target="_blank">
                <img src="images/icons/linked_in_icon.png" alt="linkedin" />
              </a>
            </li>
          </ul>
        </li>

        <li class="individual">
          <img
            src="https://media-exp1.licdn.com/dms/image/C5603AQEKm4855aKamA/profile-displayphoto-shrink_800_800/0?e=1593043200&v=beta&t=h5U5iPhDEFm0YBZ5r7-STq7x6h85KKLZ1w_iIPbTs9U"
            alt="Kelvin Wong"
          />

          <h3>Kelvin Wong</h3>

          <p>London, UK</p>

          <ul class="social-icons">
            <li>
              <a href="mailto:kjhwong@gmail.com" target="">
                <img src="images/icons/email_icon.png" alt="email" />
              </a>
            </li>

            <li>
              <a href="https://github.com/kelvinjhwong" target="_blank">
                <img src="images/icons/website_icon.png" alt="website" />
              </a>
            </li>

            <li>
              <a href="https://www.linkedin.com/in/kjhwong/" target="_blank">
                <img src="images/icons/linked_in_icon.png" alt="linkedin" />
              </a>
            </li>
          </ul>
        </li>
      </ul>
    </section>
  </body>
</html>
